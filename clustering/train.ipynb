{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cynthia/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "models=[\"mlp\",\"cnn\",\"lstm\"]\n",
    "columns=[\"readings\",\"cycle\",\"trend\"]\n",
    "files_train=os.listdir(\"dataset/train\")\n",
    "files_test=os.listdir(\"dataset/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['filled']=df['meter_reading'].apply(lambda x: 0)\n",
    "    existing_hours = df['timestamp'].dt.floor('H').unique()\n",
    "    \n",
    "    start_date = df['timestamp'].min().replace(minute=0, second=0)\n",
    "    end_date = df['timestamp'].max().replace(minute=0, second=0)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    all_hours_present = all(hour in existing_hours for hour in date_range)\n",
    "    if not(all_hours_present):\n",
    "        complete_df = pd.DataFrame({'timestamp': date_range})\n",
    "        df = complete_df.merge(df, on='timestamp', how='left')\n",
    "        df['filled']=df['filled'].fillna(1)\n",
    "        df['meter_reading'] = df['meter_reading'].interpolate(method='linear', limit_direction='both')\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    #apply minmax scaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df['meter_reading'] = scaler.fit_transform(df['meter_reading'].values.reshape(-1,1))\n",
    "    grouped_df = df.groupby(df['timestamp'].dt.date)\n",
    "    \n",
    "    # Aggregate 'meter_reading' values into a list for each day\n",
    "    aggregated_df = grouped_df.agg({'meter_reading': list, 'anomaly': list, 'filled':list}).reset_index()\n",
    "\n",
    "    # Rename columns and sort by date\n",
    "    aggregated_df.columns = ['date', 'readings', 'anomalies','filled']\n",
    "    aggregated_df = aggregated_df.sort_values(by='date')\n",
    "\n",
    "    # Display the aggregated dataframe\n",
    "    aggregated_df[\"length\"] = aggregated_df[\"readings\"].apply(lambda lst: len([x for x in lst if not pd.isna(x)]))\n",
    "    aggregated_df[\"no_anomalies\"] = aggregated_df[\"anomalies\"].apply(lambda x: True if all(val == 0 for val in x) else False)\n",
    "    aggregated_df[\"filled\"] = aggregated_df[\"filled\"].apply(lambda x: False if all(val == 0 for val in x) else True)\n",
    "\n",
    "\n",
    "    df=aggregated_df[aggregated_df[\"length\"]==24]\n",
    "    df['cycle'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[0])\n",
    "    df['trend'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[1])\n",
    "    df[\"months\"] = df[\"date\"].apply(lambda x: str(x.month))\n",
    "    df[\"weekday\"] = df[\"date\"].apply(lambda x: str(x.weekday()))\n",
    "    df[\"weekend\"] = df[\"weekday\"].apply(lambda x: 1 if x in [\"5\",\"6\"] else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461, 11)\n"
     ]
    }
   ],
   "source": [
    "train_datasets=[]\n",
    "test_datasets=[]\n",
    "\n",
    "for file in files_train:\n",
    "    dataset=pd.read_csv(\"dataset/train/\"+file)\n",
    "    train_dataset=prepare_dataset(dataset)\n",
    "    train_datasets.append(train_dataset)\n",
    "for file in files_test:\n",
    "    dataset=pd.read_csv(\"dataset/test/\"+file)\n",
    "    test_dataset=prepare_dataset(dataset)\n",
    "    test_datasets.append(test_dataset)\n",
    "# concatenate all the csv files into one\n",
    "train_dataset= pd.concat(train_datasets)\n",
    "test_dataset= pd.concat(test_datasets)\n",
    "\n",
    "train_dataset=train_dataset[train_dataset[\"filled\"]==0]\n",
    "test_dataset=test_dataset[test_dataset[\"filled\"]==0]\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>readings</th>\n",
       "      <th>anomalies</th>\n",
       "      <th>filled</th>\n",
       "      <th>length</th>\n",
       "      <th>no_anomalies</th>\n",
       "      <th>cycle</th>\n",
       "      <th>trend</th>\n",
       "      <th>months</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>[0.4949748743718593, 0.5678391959798995, 0.522...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.03123344778510584, 0.03941239826478582, 0....</td>\n",
       "      <td>[0.5262083221569651, 0.5284267977151137, 0.515...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>[0.4396984924623116, 0.4271356783919598, 0.487...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0018269408516842778, -0.021690685703229518,...</td>\n",
       "      <td>[0.4378715516106273, 0.44882636409518933, 0.46...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>[0.41959798994974873, 0.4396984924623116, 0.52...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.009053880414004123, -0.01316737908346971, ...</td>\n",
       "      <td>[0.42865187036375285, 0.4528658715457813, 0.47...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>[0.42462311557788945, 0.4296482412060301, 0.42...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.0003693508051154648, 0.0022657521224360155...</td>\n",
       "      <td>[0.4249924663830049, 0.4273824890835941, 0.429...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>[0.5477386934673367, 0.5402010050251257, 0.545...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.0027492076873769644, -0.004783365280123131...</td>\n",
       "      <td>[0.5504879011547137, 0.5449843703052488, 0.538...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                           readings  \\\n",
       "0  2016-01-01  [0.4949748743718593, 0.5678391959798995, 0.522...   \n",
       "1  2016-01-02  [0.4396984924623116, 0.4271356783919598, 0.487...   \n",
       "2  2016-01-03  [0.41959798994974873, 0.4396984924623116, 0.52...   \n",
       "3  2016-01-04  [0.42462311557788945, 0.4296482412060301, 0.42...   \n",
       "4  2016-01-05  [0.5477386934673367, 0.5402010050251257, 0.545...   \n",
       "\n",
       "                                           anomalies  filled  length  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   False      24   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   False      24   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   False      24   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   False      24   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   False      24   \n",
       "\n",
       "   no_anomalies                                              cycle  \\\n",
       "0          True  [-0.03123344778510584, 0.03941239826478582, 0....   \n",
       "1          True  [0.0018269408516842778, -0.021690685703229518,...   \n",
       "2          True  [-0.009053880414004123, -0.01316737908346971, ...   \n",
       "3          True  [-0.0003693508051154648, 0.0022657521224360155...   \n",
       "4          True  [-0.0027492076873769644, -0.004783365280123131...   \n",
       "\n",
       "                                               trend months weekday  weekend  \n",
       "0  [0.5262083221569651, 0.5284267977151137, 0.515...      1       4        0  \n",
       "1  [0.4378715516106273, 0.44882636409518933, 0.46...      1       5        1  \n",
       "2  [0.42865187036375285, 0.4528658715457813, 0.47...      1       6        1  \n",
       "3  [0.4249924663830049, 0.4273824890835941, 0.429...      1       0        0  \n",
       "4  [0.5504879011547137, 0.5449843703052488, 0.538...      1       1        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_numpy(column):\n",
    "    dataset=np.array([np.array(i) for i in column])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary classifier model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,class_weights = [1, 5]):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0) \n",
    "        print(x.shape)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def calculate_loss(self, outputs, targets):\n",
    "        # Use weighted binary cross-entropy loss\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=self.class_weights)\n",
    "        loss = criterion(outputs, targets)\n",
    "        return loss\n",
    "    \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states to zero\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Extract the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Apply sigmoid activation function\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (1571649736.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[189], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    if x=[1,0]:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "def labelize(arr):\n",
    "    arr2=[]\n",
    "    for x in arr:\n",
    "        if x=[1,0]:\n",
    "            arr2.append([1.,0.])\n",
    "        else:\n",
    "            arr2.append([0.,1.])\n",
    "    return np.array(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 1.0 appears 1 times.\n",
      "Value 0.0 appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Your list of values\n",
    "my_list = y_train.detach().numpy()[0].tolist()\n",
    "\n",
    "# Count the occurrences of each value\n",
    "value_counts = Counter(my_list)\n",
    "\n",
    "# Print the result\n",
    "for value, count in value_counts.items():\n",
    "    print(f\"Value {value} appears {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.542553191489361"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1367/94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(x):\n",
    "    \n",
    "    if x==1:\n",
    "        return(np.array([1,0]))\n",
    "    else:\n",
    "        return(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=train_dataset[\"no_anomalies\"].apply(lambda x: np.array([1,0]) if x==True else np.array([0,1]))\n",
    "labels_y=[]\n",
    "for x in y_train:\n",
    "    labels_y.append(x)\n",
    "labels_y=np.array(labels_y)\n",
    "labels_y=torch.from_numpy(labels_y).float()\n",
    "labels_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [1, 0]\n",
       "1      [1, 0]\n",
       "2      [1, 0]\n",
       "3      [1, 0]\n",
       "4      [1, 0]\n",
       "        ...  \n",
       "361    [1, 0]\n",
       "362    [1, 0]\n",
       "363    [1, 0]\n",
       "364    [1, 0]\n",
       "365    [1, 0]\n",
       "Name: no_anomalies, Length: 1461, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1461]) torch.Size([24, 1452]) torch.Size([1461, 2]) torch.Size([1452, 2])\n",
      "torch.Size([1461, 24])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1461, 2])) must be the same as input size (torch.Size([1461, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     34\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     outputs \u001b[39m=\u001b[39m model(X_train)\n\u001b[0;32m---> 36\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcalculate_loss(outputs, y_train)\n\u001b[1;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(outputs)\n\u001b[1;32m     38\u001b[0m     \u001b[39m# Backward pass and optimization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[188], line 30\u001b[0m, in \u001b[0;36mBinaryClassifier.calculate_loss\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_loss\u001b[39m(\u001b[39mself\u001b[39m, outputs, targets):\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Use weighted binary cross-entropy loss\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_weights)\n\u001b[0;32m---> 30\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:714\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 714\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    715\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    716\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    717\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3148\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3145\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3150\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1461, 2])) must be the same as input size (torch.Size([1461, 1]))"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "for model_n in models:\n",
    "    for column in columns:\n",
    "        X=train_dataset[column]\n",
    "        X_train=torch.Tensor(make_numpy(X)).transpose(0,1)\n",
    "        X_test=torch.Tensor(make_numpy(test_dataset[column])).transpose(0,1)\n",
    "        y_train=train_dataset[\"no_anomalies\"].apply(lambda x: np.array([1,0]) if x==True else np.array([0,1]))\n",
    "        labels_y=[]\n",
    "        for x in y_train:\n",
    "            labels_y.append(x)\n",
    "        labels_y=np.array(labels_y)\n",
    "        labels_y=torch.from_numpy(labels_y).float()\n",
    "        y_train=labels_y\n",
    "\n",
    "        y_test=test_dataset[\"no_anomalies\"].apply(lambda x: np.array([1,0]) if x==True else np.array([0,1]))\n",
    "        labels_y=[]\n",
    "        for x in y_test:\n",
    "            labels_y.append(x)\n",
    "        labels_y=np.array(labels_y)\n",
    "        labels_y=torch.from_numpy(labels_y).float()\n",
    "        y_test=labels_y\n",
    "\n",
    "        num_epochs = 10000\n",
    "        print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "        if model_n==\"mlp\":\n",
    "\n",
    "            input_size = 24\n",
    "            hidden_size1 = 18\n",
    "            hidden_size2 = 8\n",
    "            model = BinaryClassifier(input_size, hidden_size1, hidden_size2)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            for epoch in range(num_epochs):\n",
    "                # Forward pass\n",
    "                outputs = model(X_train)\n",
    "                loss = model.calculate_loss(outputs, y_train)\n",
    "                print(outputs)\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if epoch == 0:\n",
    "                    best_val_loss = loss\n",
    "                    best_model = model\n",
    "                # Print loss every 100 epochs\n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "                if (epoch + 1) % 20 == 0:\n",
    "                    outputs = model(X_test)\n",
    "                    loss = model.calculate_loss(outputs, y_test)\n",
    "                    #print(f'Test Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "                    best_val_loss = min(loss,best_val_loss)\n",
    "                    if loss==best_val_loss:\n",
    "                        best_model = model\n",
    "                        #print(\"best model updated at epoch \",epoch+1)\n",
    "                        #print(outputs.squeeze(1).detach().numpy())\n",
    "                        threshold = 0.5\n",
    "                        predicted_classes = (outputs > threshold).float()\n",
    "                        best_kappa=cohen_kappa_score(predicted_classes.squeeze(1).detach().numpy(), y_test.squeeze(0).detach().numpy())\n",
    "            #save model\n",
    "            torch.save(best_model.state_dict(), \"models/\"+model_n+\"_\"+column+\"_kappa_\"+str(best_kappa)+\".pt\")\n",
    "            print(\"best model saved, kappa = \",best_kappa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelize(outputs.squeeze(1).detach().numpy()), y_test.squeeze(0).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(y_train.squeeze(1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5036],\n",
       "        [0.5023],\n",
       "        [0.5012],\n",
       "        ...,\n",
       "        [0.5202],\n",
       "        [0.5203],\n",
       "        [0.5203]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelize(outputs.squeeze(1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
