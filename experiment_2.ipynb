{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cynthia/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler, StandardScaler,MaxAbsScaler,PowerTransformer,QuantileTransformer\n",
    "import statsmodels.api as sm\n",
    "from get_model import get_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['filled']=df['meter_reading'].apply(lambda x: 0)\n",
    "    existing_hours = df['timestamp'].dt.floor('H').unique()\n",
    "\n",
    "    start_date = df['timestamp'].min().replace(minute=0, second=0)\n",
    "    end_date = df['timestamp'].max().replace(minute=0, second=0)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    all_hours_present = all(hour in existing_hours for hour in date_range)\n",
    "    if not(all_hours_present):\n",
    "        complete_df = pd.DataFrame({'timestamp': date_range})\n",
    "        df = complete_df.merge(df, on='timestamp', how='left')\n",
    "        df['filled']=df['filled'].fillna(1)\n",
    "        df['meter_reading'] = df['meter_reading'].interpolate(method='linear', limit_direction='both')\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    #apply minmax scaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df['meter_reading'] = scaler.fit_transform(df['meter_reading'].values.reshape(-1,1))\n",
    "    grouped_df = df.groupby(df['timestamp'].dt.date)\n",
    "\n",
    "    # Aggregate 'meter_reading' values into a list for each day\n",
    "    aggregated_df = grouped_df.agg({'meter_reading': list, 'anomaly': list, 'filled':list}).reset_index()\n",
    "\n",
    "    # Rename columns and sort by date\n",
    "    aggregated_df.columns = ['date', 'readings', 'anomalies','filled']\n",
    "    aggregated_df = aggregated_df.sort_values(by='date')\n",
    "\n",
    "    # Display the aggregated dataframe\n",
    "    aggregated_df[\"length\"] = aggregated_df[\"readings\"].apply(lambda lst: len([x for x in lst if not pd.isna(x)]))\n",
    "    aggregated_df[\"no_anomalies\"] = aggregated_df[\"anomalies\"].apply(lambda x: True if all(val == 0 for val in x) else False)\n",
    "\n",
    "    df=aggregated_df[aggregated_df[\"length\"]==24]\n",
    "    df['cycle'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[0])\n",
    "    df['trend'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[1])\n",
    "    df[\"months\"] = df[\"date\"].apply(lambda x: str(x.month))\n",
    "    df[\"weekday\"] = df[\"date\"].apply(lambda x: str(x.weekday()))\n",
    "    df[\"weekend\"] = df[\"weekday\"].apply(lambda x: 1 if x in [\"5\",\"6\"] else 0)\n",
    "    return df\n",
    "\n",
    "def generate_data(df,column):\n",
    "    data=df[column]\n",
    "    data=np.concatenate(data)\n",
    "\n",
    "    data=data.reshape((1,df.shape[0],24))\n",
    "\n",
    "    return data\n",
    "\n",
    "def prepare_line(col1, col2,col3):\n",
    "    columns=[col1, col2,col3]\n",
    "    dataset=[]\n",
    "    for column in columns:\n",
    "        data=np.array(column)\n",
    "        data=data.reshape(1,1,24)\n",
    "        dataset.append(data)\n",
    "    data_array=np.concatenate(dataset)\n",
    "    data_array=np.transpose(data_array, (1,0,2))\n",
    "    return(data_array)\n",
    "def prepare_line0(col1, col2):\n",
    "    columns=[col1, col2]\n",
    "    dataset=[]\n",
    "    for column in columns:\n",
    "        data=np.array(column)\n",
    "        data=data.reshape(1,1,24)\n",
    "        dataset.append(data)\n",
    "    data_array=np.concatenate(dataset)\n",
    "    data_array=np.transpose(data_array, (1,0,2))\n",
    "    return(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "def get_cluster_labels(df,column):\n",
    "    def get_majority(lst):\n",
    "        return max(set(lst), key=lst.count)\n",
    "    X = np.array(df[column].tolist())\n",
    "    # do PCA with 2 components on X\n",
    "    pca = PCA(n_components=2)\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(X)\n",
    "    max_label = get_majority(kmeans.labels_.tolist())\n",
    "    labels=[0 if i==max_label else 1 for i in kmeans.labels_ ]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files=os.listdir('dataset/train')[0:5]\n",
    "import io\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "captured_output = io.StringIO()\n",
    "files=[\"118.csv\",\"246.csv\",\"1245.csv\",\"1311.csv\",\"1141.csv\"]\n",
    "models=['AE',\n",
    " 'VAE',\n",
    " 'BETA',\n",
    " 'VAE_LinNF',\n",
    " 'VAE_IAF',\n",
    " 'DBVAE',\n",
    " 'IWVAE',\n",
    " 'MIWAE',\n",
    " 'CIWAE',\n",
    " 'WAE',\n",
    "'INFOVAE',\n",
    " 'VAMP',\n",
    " 'SVAE',\n",
    " 'PVAE',\n",
    " 'VQVAE',\n",
    " 'HVAE',\n",
    " 'RAE_GP',\n",
    " 'RHVAE']\n",
    "pbar=tqdm(total=len(models))\n",
    "try:\n",
    "    for model_name in models:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = captured_output\n",
    "        for file in files:\n",
    "            \n",
    "            dataset=pd.read_csv('dataset/train/'+file)\n",
    "            dataset=prepare_dataset(dataset)\n",
    "            columns_cluster=[\"cycle\",\"trend\"]\n",
    "                        \n",
    "            for column_cluster in columns_cluster:\n",
    "                dataset['kmeans_{}'.format(column_cluster)]=np.nan\n",
    "            for month in dataset[\"months\"].unique():\n",
    "                for day in dataset[\"weekend\"].unique():\n",
    "                        df=dataset[(dataset[\"months\"]==month) & (dataset[\"weekend\"]==day)].copy()\n",
    "                        \n",
    "                        \n",
    "                        for column_cluster in columns_cluster:\n",
    "                                \n",
    "                                df['kmeans_{}'.format(column_cluster)]=get_cluster_labels(df,column_cluster)\n",
    "                                #create a new column with the cluster labels in dataset and initialize it with NAN\n",
    "                                \n",
    "                                \n",
    "                        dataset.update(df)\n",
    "            for column_cluster in columns_cluster:\n",
    "                if not(os.path.exists(\"experiment_2/csv/{}\".format(column_cluster))):\n",
    "                    os.makedirs(\"experiment_2/csv/{}\".format(column_cluster))\n",
    "                train=dataset[dataset['kmeans_{}'.format(column_cluster)]==0.0]\n",
    "                test=dataset[dataset['kmeans_{}'.format(column_cluster)]==1.0]\n",
    "                for month in dataset[\"months\"].unique():\n",
    "                    for day in dataset[\"weekend\"].unique():\n",
    "                        \n",
    "                        train_data=train[(train[\"months\"]==month) & (train[\"weekend\"]==day)]\n",
    "                        test_data=test[(test[\"months\"]==month) & (test[\"weekend\"]==day)]\n",
    "                        train_data.reset_index(inplace=True)\n",
    "                        test_data.reset_index(inplace=True) \n",
    "                        if train_data.shape[0]==0:\n",
    "                            split=0.5\n",
    "                            train_data=test_data[0:int(split*test_data.shape[0])]\n",
    "                            eval_data=test_data[int(split*test_data.shape[0]):]\n",
    "                        else:\n",
    "                            if test_data.shape[0]==0:\n",
    "                                eval_data=None\n",
    "                            else:\n",
    "                                eval_data=test_data\n",
    "                        train_columns=[\"readings\",\"cycle\",\"trend\"]\n",
    "                        train_numpy=np.concatenate([generate_data(train_data,column) for column in train_columns])\n",
    "                        train_numpy=train_numpy.transpose((1,0,2))\n",
    "                        \n",
    "                        pipeline = get_model(model_name,dim=len(train_columns),train_batch=train_numpy.shape[0])\n",
    "                        pipeline(\n",
    "                        train_data=train_numpy# must be torch.Tensor, np.array or torch datasets\n",
    "                        )\n",
    "                        print(\"Model ready!\")\n",
    "                        my_vae_model=pipeline.model\n",
    "                        train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
    "                        train_data.reset_index(inplace=True, drop=True)\n",
    "                        train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
    "\n",
    "                        if test_data.shape[0]!=0:\n",
    "                            eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
    "                            eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
    "                            eval_data.reset_index(inplace=True, drop=True)\n",
    "                        if test_data.shape[0] != 0:\n",
    "                            dataset_final=pd.concat([train_data,eval_data])\n",
    "                        else:\n",
    "                            dataset_final=train_data\n",
    "                        dataset_final[\"reconstruction\"]=dataset_final[\"reconstruction\"].apply((lambda x: x.reshape((1, len(train_columns), 24))))\n",
    "                        dataset_final['difference'] = dataset_final['preprocessed'] - dataset_final['reconstruction']\n",
    "\n",
    "                        for i, column in enumerate(train_columns):\n",
    "                            dataset_final['difference_norm_{}'.format(column)] = dataset_final['difference'].apply(lambda x: np.linalg.norm(x[:,i,:],axis=(0)))\n",
    "                        dataset_final.reset_index(inplace=True,drop=True)\n",
    "                        for ind in dataset_final.index:\n",
    "                            tmp_df=pd.DataFrame()\n",
    "                            anomalies=dataset_final.loc[ind,\"anomalies\"]\n",
    "                            filled=dataset_final.loc[ind,\"filled\"]\n",
    "                            tmp_df[\"anomalies\"]=anomalies\n",
    "                            tmp_df[\"filled\"]=filled\n",
    "                            tmp_df.reset_index(inplace=True,drop=True)\n",
    "                            date=dataset_final.loc[ind,\"date\"]\n",
    "                            tmp_df['datetime'] = pd.to_datetime(date) + pd.to_timedelta(tmp_df.index, unit='h')\n",
    "\n",
    "\n",
    "                            \n",
    "                            for column in train_columns:\n",
    "\n",
    "                                tmp_df[\"difference_norm_{}\".format(column)]=dataset_final.loc[ind,\"difference_norm_{}\".format(column)]\n",
    "                                \n",
    "                            if not(os.path.exists(\"experiment_2/csv/{}/latent_dim_16/{}\".format(column_cluster,model_name))):\n",
    "                                os.makedirs(\"experiment_2/csv/{}/latent_dim_16/{}\".format(column_cluster,model_name))\n",
    "                            tmp_df.to_csv(\"experiment_2/csv/{}/latent_dim_16/{}/building_{}_month_{}_weekend_{}_{}_column_{}.csv\".format(column_cluster,model_name,file,month,day,ind,column),index=False)\n",
    "                clear_output(wait=False)    \n",
    "        sys.stdout = original_stdout\n",
    "        pbar.update(1)\n",
    "except Exception as e:\n",
    "    sys.stdout = original_stdout\n",
    "    print(e)\n",
    "    print(\"problem with this case:\")\n",
    "    print(\"model: {}, building: {}, month: {}, day: {}, index: {}, column: {}\".format(model_name,file,month,day,ind,column))\n",
    "    print(\"training samples: {}, testing samples: {}\".format(train_data.shape[0],eval_data.shape[0]))\n",
    "    print(\"try removing the model from the model list and debug it separately\")\n",
    "    pbar.close()\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
