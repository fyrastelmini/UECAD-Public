{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cynthia/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import minmaxx scaler\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler, StandardScaler,MaxAbsScaler,PowerTransformer,QuantileTransformer\n",
    "import statsmodels.api as sm\n",
    "from get_model import get_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['filled']=df['meter_reading'].apply(lambda x: 0)\n",
    "    existing_hours = df['timestamp'].dt.floor('H').unique()\n",
    "\n",
    "    start_date = df['timestamp'].min().replace(minute=0, second=0)\n",
    "    end_date = df['timestamp'].max().replace(minute=0, second=0)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    all_hours_present = all(hour in existing_hours for hour in date_range)\n",
    "    if not(all_hours_present):\n",
    "        complete_df = pd.DataFrame({'timestamp': date_range})\n",
    "        df = complete_df.merge(df, on='timestamp', how='left')\n",
    "        df['filled']=df['filled'].fillna(1)\n",
    "        df['meter_reading'] = df['meter_reading'].interpolate(method='linear', limit_direction='both')\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    #interpolate the readings\n",
    "    #df['meter_reading'] = df['meter_reading'].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    #apply minmax scaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df['meter_reading'] = scaler.fit_transform(df['meter_reading'].values.reshape(-1,1))\n",
    "    grouped_df = df.groupby(df['timestamp'].dt.date)\n",
    "\n",
    "    # Step 3: Aggregate 'meter_reading' values into a list for each day\n",
    "    aggregated_df = grouped_df.agg({'meter_reading': list, 'anomaly': list, 'filled':list}).reset_index()\n",
    "\n",
    "    # Step 4: Rename columns and sort by date\n",
    "    aggregated_df.columns = ['date', 'readings', 'anomalies','filled']\n",
    "    aggregated_df = aggregated_df.sort_values(by='date')\n",
    "\n",
    "    # Display the aggregated dataframe\n",
    "    aggregated_df[\"length\"] = aggregated_df[\"readings\"].apply(lambda lst: len([x for x in lst if not pd.isna(x)]))\n",
    "    aggregated_df[\"no_anomalies\"] = aggregated_df[\"anomalies\"].apply(lambda x: True if all(val == 0 for val in x) else False)\n",
    "    #aggregated_df.to_csv(\"LEAD/buildings/building_{}.csv\".format(i),index=False)\n",
    "    df=aggregated_df[aggregated_df[\"length\"]==24]\n",
    "    df['cycle'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[0])\n",
    "    df['trend'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[1])\n",
    "    df[\"months\"] = df[\"date\"].apply(lambda x: str(x.month))\n",
    "    df[\"weekday\"] = df[\"date\"].apply(lambda x: str(x.weekday()))\n",
    "    df[\"weekend\"] = df[\"weekday\"].apply(lambda x: 1 if x in [\"5\",\"6\"] else 0)\n",
    "    return df\n",
    "\n",
    "def generate_data(df,column):\n",
    "    data=df[column]\n",
    "    data=np.concatenate(data)\n",
    "    #tmp=data[0:24]\n",
    "    data=data.reshape((1,df.shape[0],24))\n",
    "    #print(data[0,0,:]==tmp)\n",
    "    return data\n",
    "\n",
    "def prepare_line(col1, col2,col3):\n",
    "    columns=[col1, col2,col3]\n",
    "    dataset=[]\n",
    "    for column in columns:\n",
    "        data=np.array(column)\n",
    "        data=data.reshape(1,1,24)\n",
    "        dataset.append(data)\n",
    "    data_array=np.concatenate(dataset)\n",
    "    data_array=np.transpose(data_array, (1,0,2))\n",
    "    #check=np.concatenate([col1,col2,col3])\n",
    "    #check=check.reshape(1,3,24)\n",
    "    #print(check==data_array)\n",
    "    return(data_array)\n",
    "def prepare_line0(col1, col2):\n",
    "    columns=[col1, col2]\n",
    "    dataset=[]\n",
    "    for column in columns:\n",
    "        data=np.array(column)\n",
    "        data=data.reshape(1,1,24)\n",
    "        dataset.append(data)\n",
    "    data_array=np.concatenate(dataset)\n",
    "    data_array=np.transpose(data_array, (1,0,2))\n",
    "    #check=np.concatenate([col1,col2,col3])\n",
    "    #check=check.reshape(1,3,24)\n",
    "    #print(check==data_array)\n",
    "    return(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "def get_cluster_labels(df,column):\n",
    "    def get_majority(lst):\n",
    "        return max(set(lst), key=lst.count)\n",
    "    X = np.array(df[column].tolist())\n",
    "    # do PCA with 2 components on X\n",
    "    pca = PCA(n_components=2)\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(X)\n",
    "    max_label = get_majority(kmeans.labels_.tolist())\n",
    "    labels=[0 if i==max_label else 1 for i in kmeans.labels_ ]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-38. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 11\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd548eb0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 19.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.7323\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 17.77batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.8486\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 18.20batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.8441\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 17.97batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.0968\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 17.61batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.8277\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 19.67batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.7669\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 17.12batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.1313\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 11.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.251\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 18.75batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 106.9925\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 17.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.2784\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 19.12batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.5539\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 19.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 120.0592\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 17.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.5015\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 18.38batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.8909\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 19.49batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.3763\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 19.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.3306\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 19.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.6462\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 18.87batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 97.0476\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 18.21batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.4408\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 19.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.7227\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 19.28batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.0215\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 18.74batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.9108\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 19.07batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.1221\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 11.16batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.9177\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 18.75batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.8626\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 19.04batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.3367\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 17.49batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.9213\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 18.15batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.5916\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 18.00batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.7671\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 17.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 58.2243\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 17.47batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.0438\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 17.83batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.9023\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 17.77batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.6441\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 17.19batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.6985\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 18.52batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.9931\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 15.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.1259\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 11.90batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 106.0618\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 18.69batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.7909\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 14.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.7165\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00,  7.10batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.6808\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 12.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.6266\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 11.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.4154\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 18.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 88.6838\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 18.11batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.8881\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 18.51batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.9676\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 17.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.3834\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 16.84batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.4696\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 17.82batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 61.6026\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 16.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 103.8873\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 18.28batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.9383\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-38/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-42. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 6\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd5485e0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 14.55batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.3959\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 15.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 50.2268\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 18.74batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 58.0825\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 18.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 104.7784\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 18.29batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.2584\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 18.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.4459\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 11.19batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.828\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 18.72batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 58.7672\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 16.76batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.7954\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 18.83batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.2017\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 18.80batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.6168\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 19.04batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 149.1301\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 17.69batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.4544\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 17.60batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.5646\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 18.95batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.8307\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 18.47batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.7221\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 18.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.3436\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 17.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.0113\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 19.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 106.0126\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 17.15batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.5463\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 19.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 75.6141\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 11.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.0599\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00,  9.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.7648\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 18.63batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 53.1822\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 19.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.8102\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 17.31batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.3081\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 18.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 116.2375\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 17.90batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.89\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 18.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 101.8333\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 18.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 49.9701\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 18.89batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 55.3585\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 19.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.4633\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 17.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 141.083\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 18.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.4282\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 18.91batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.2371\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 12.79batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.5999\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 17.19batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.7651\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 11.71batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 111.299\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 18.38batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.0954\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 17.99batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.2146\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 18.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.7239\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 19.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.2529\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 18.16batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 97.7239\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 17.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.2847\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 18.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.1627\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 18.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.0463\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 11.50batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.9356\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 17.40batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.5065\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 18.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 61.2032\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 17.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 64.0662\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-42/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-45. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 13\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffc9ebca0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 12.87batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.489\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00,  8.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.4481\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 16.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 88.6305\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 17.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.6543\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 10.99batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 115.0398\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 16.90batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 101.927\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 16.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.3669\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 17.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.423\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 17.50batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.7909\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 16.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.0269\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 16.15batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 88.8546\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 18.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.8632\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 17.33batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.9134\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 13.46batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.9032\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 16.60batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.8419\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 17.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.1267\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 18.18batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.1835\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 17.31batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.5073\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 17.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.9903\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 16.12batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 105.6959\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 18.33batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.7068\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 17.27batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.2144\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 10.42batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.3339\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00,  7.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.0665\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 16.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.5417\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 16.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.3736\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 17.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.5906\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 16.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.5564\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 18.15batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.9632\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 19.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.2466\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 10.89batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.1623\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 17.25batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.6735\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 17.63batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.1806\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 17.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 75.1581\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 17.84batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.5482\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 17.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.5056\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 17.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.6519\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 17.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.0437\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 12.26batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 75.6781\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 17.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.5704\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 17.09batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 75.8793\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 17.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.0309\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 17.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.4978\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 17.67batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.9573\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 17.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.0675\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 17.91batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.6339\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 11.74batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.1863\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 16.60batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.6675\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 15.16batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.1462\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 11.82batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.6498\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-45/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-49. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 4\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd54aad0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 16.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 52.6369\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 10.71batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 51.2478\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 18.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 57.0187\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 17.46batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.2584\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 17.63batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 113.4731\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 18.70batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 60.0679\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 18.25batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.9115\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 18.23batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 59.4479\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 11.54batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 54.7969\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 18.00batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.5201\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 18.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 111.427\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 17.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.2403\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 18.38batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 57.507\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 18.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.867\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 16.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 42.1414\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 12.42batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.0099\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 17.08batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.1136\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 18.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 62.9439\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 18.42batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.4098\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00,  8.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 104.6513\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 17.55batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.5902\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 16.20batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 53.4015\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 15.87batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 66.7502\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 15.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 100.2815\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 14.84batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.1634\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 18.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 114.4213\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 16.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.8992\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 17.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.1569\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 18.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.5194\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00,  6.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.4286\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 16.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.2031\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 17.31batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.2583\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 19.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 55.4588\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 18.02batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.9422\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 11.84batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.4581\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 17.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.2837\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 17.42batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.5316\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 16.74batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 60.3942\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 15.86batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 178.1438\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 18.17batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 109.8042\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 15.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 61.9169\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 14.25batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 104.5748\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 14.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 109.2825\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 14.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 105.7344\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 16.60batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 122.7602\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 17.26batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 117.6991\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00,  6.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 64.0631\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 17.54batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 34.0789\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 18.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 114.8526\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 17.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.0025\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-49/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-53. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 9\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd548fa0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 15.00batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.5007\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 17.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 88.9701\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 17.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.6257\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 18.67batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.8147\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 12.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.4226\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00,  9.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.7521\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 17.69batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 113.1792\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 18.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.6795\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 18.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.7582\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 18.02batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.9757\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 12.69batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.3537\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 18.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.2859\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 17.70batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.8538\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 17.46batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.1067\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 17.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 102.562\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 17.97batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.1295\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 12.38batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.5174\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 18.20batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 109.3611\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 18.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.018\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 18.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 100.3437\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 18.83batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.2054\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00,  8.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.8595\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 16.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.5185\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 16.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.9293\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 16.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.0489\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 16.93batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.373\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 16.27batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.0422\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00,  7.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.0522\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 16.34batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.0016\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 17.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.5305\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 17.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.8724\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 16.60batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 88.851\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 16.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 66.6014\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 17.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.0225\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 17.33batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.595\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 18.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.8983\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 14.28batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.593\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 15.51batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.7774\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00,  7.79batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.0756\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 16.97batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.4371\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 15.90batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.3755\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 13.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.1846\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 17.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.9564\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 17.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.1192\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 18.10batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.1293\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 17.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 66.5512\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 11.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.2637\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 16.91batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.8545\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00,  8.27batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.6435\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 17.69batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.3666\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-53/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-56. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 3\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd54aef0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 16.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.7629\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 16.40batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.8994\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 18.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 138.3956\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 17.76batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 129.429\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 16.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.5369\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00,  6.90batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.2229\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 14.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 42.5674\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 17.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 32.1722\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 19.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 100.5213\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 15.46batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 57.3954\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 12.85batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.8783\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 17.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 46.8882\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 16.10batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 144.7599\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 15.87batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 61.9326\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 10.61batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.2627\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 10.97batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 66.902\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 18.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.9269\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 18.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.087\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 17.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 126.7635\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 18.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.445\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 11.23batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 117.9354\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 18.02batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 64.3444\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 17.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.8063\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 18.15batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.173\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 19.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 54.1926\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 13.32batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 54.3619\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 17.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 51.1094\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 18.75batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 35.8995\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 16.96batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 30.8384\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00,  8.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 49.2483\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 11.89batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.0256\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 16.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 135.1551\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 17.69batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.7923\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 16.88batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 116.4394\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 17.40batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 8.4935\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 11.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.7325\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 18.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 39.8919\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 17.97batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.2108\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 16.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 97.4523\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 18.33batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 36.8707\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 18.85batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 103.9124\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 11.12batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 53.9216\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 18.09batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 109.9717\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 18.26batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.4997\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 17.21batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 58.3658\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00,  8.32batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.0348\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 15.89batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 86.6298\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 15.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.3984\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 16.77batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 34.4339\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 17.33batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.1402\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-44-56/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-00. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 14\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd54aad0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 11.38batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.6372\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 17.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.5069\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 16.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.3889\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 15.12batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.5352\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 17.83batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.7299\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 13.96batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.4873\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00,  7.05batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.0045\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 16.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.9025\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 17.70batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.2515\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 18.19batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 75.6983\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 16.09batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.8753\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 12.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 96.6805\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 15.76batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.6413\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 16.82batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.7608\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00,  6.74batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.1985\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 16.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.5509\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 18.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.2997\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 17.25batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.2019\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 12.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.9115\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 16.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.1843\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 16.46batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.9928\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00,  7.99batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 95.8221\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 17.85batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.7807\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00,  9.51batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.4615\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 17.54batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.3917\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 16.42batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.628\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 17.30batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.8125\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 17.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 53.5938\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 11.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.1485\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 17.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.4388\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 15.49batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.8363\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 14.95batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.8172\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00,  9.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.0558\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 15.63batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.5756\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 13.51batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.3242\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 17.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.2216\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 16.79batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.4408\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 16.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.6445\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 11.72batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.7729\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 16.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.6962\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 15.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.6046\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 11.34batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.1505\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 14.71batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.765\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 15.86batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.4925\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 18.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 73.6099\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 18.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 75.6161\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 10.89batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.9899\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 18.03batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 88.7684\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 16.47batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.8453\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 16.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.4415\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-00/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-04. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 7\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd54bc70>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 16.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.0517\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 16.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.7423\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 15.40batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.7466\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 14.72batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.5636\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 16.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.7602\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 11.75batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 61.4922\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 17.65batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.1457\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 17.20batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 134.0162\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 17.63batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.2316\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 10.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.2915\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 16.99batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.632\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 17.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.1246\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 16.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.2626\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 16.05batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 60.1285\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 18.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 82.3153\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 14.50batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.8226\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 15.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 110.9862\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 17.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.1449\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 11.07batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.1915\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 16.65batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.4694\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 15.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 46.797\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 18.34batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.7255\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 15.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 61.6632\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 17.39batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 101.3034\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 18.00batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.0311\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 18.91batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.8746\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 15.39batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.5266\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 14.80batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 58.3308\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00, 18.72batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.0292\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 16.93batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.4064\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 11.05batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.3946\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 17.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 63.487\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 16.34batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.4963\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 14.95batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.008\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 14.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 58.5114\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 17.75batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.2381\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 11.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 47.8086\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 17.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.8741\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 17.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 46.7311\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 17.49batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 44.8057\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00, 10.30batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 55.092\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 17.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.6661\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 16.84batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.404\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00,  8.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 46.2055\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 16.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.4906\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 16.44batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.9684\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 18.88batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 48.2272\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00, 15.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.3017\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00,  9.68batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.2624\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 17.44batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.0465\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-04/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-08. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 10\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd5a0dc0>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00, 13.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.4081\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00,  6.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.6746\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 10.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.8797\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 10.95batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.4589\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 15.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 78.2867\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 16.82batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.9583\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 16.71batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.0957\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00, 11.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 114.3173\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 18.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 97.8109\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 17.57batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 74.0874\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 12.21batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.4726\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 16.71batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 93.8707\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 16.86batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.9223\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00, 18.20batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.6479\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 10.28batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.3282\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00,  8.09batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 99.8543\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 16.58batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 64.0189\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 16.16batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.3918\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 15.77batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.6087\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00, 17.96batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.9531\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 17.66batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.0221\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00,  6.30batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.3422\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 15.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.8234\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 13.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.7234\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 14.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.428\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 17.10batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.9641\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 17.17batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.4762\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/50: 100%|██████████| 1/1 [00:00<00:00, 11.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.2284\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/50: 100%|██████████| 1/1 [00:00<00:00,  8.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.5612\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/50: 100%|██████████| 1/1 [00:00<00:00, 17.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.7945\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 31/50: 100%|██████████| 1/1 [00:00<00:00, 17.64batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 94.3506\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 32/50: 100%|██████████| 1/1 [00:00<00:00, 14.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.044\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 33/50: 100%|██████████| 1/1 [00:00<00:00, 16.52batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 90.1501\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 34/50: 100%|██████████| 1/1 [00:00<00:00, 18.11batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.7912\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 35/50: 100%|██████████| 1/1 [00:00<00:00, 11.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 66.8811\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 36/50: 100%|██████████| 1/1 [00:00<00:00, 18.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.2467\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 37/50: 100%|██████████| 1/1 [00:00<00:00, 17.22batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 84.7971\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 38/50: 100%|██████████| 1/1 [00:00<00:00, 17.77batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 97.1005\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 39/50: 100%|██████████| 1/1 [00:00<00:00, 10.86batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.5126\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 40/50: 100%|██████████| 1/1 [00:00<00:00, 15.93batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.0196\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 41/50: 100%|██████████| 1/1 [00:00<00:00,  9.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.9671\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 42/50: 100%|██████████| 1/1 [00:00<00:00, 17.00batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 77.68\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 43/50: 100%|██████████| 1/1 [00:00<00:00, 11.15batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.5126\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 44/50: 100%|██████████| 1/1 [00:00<00:00, 17.65batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 87.7397\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 45/50: 100%|██████████| 1/1 [00:00<00:00, 16.95batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 83.7642\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 46/50: 100%|██████████| 1/1 [00:00<00:00, 18.17batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.2726\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 47/50: 100%|██████████| 1/1 [00:00<00:00, 11.56batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.3049\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 48/50: 100%|██████████| 1/1 [00:00<00:00,  8.18batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 81.2833\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 49/50: 100%|██████████| 1/1 [00:00<00:00, 16.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 85.2436\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 50/50: 100%|██████████| 1/1 [00:00<00:00, 13.09batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.5216\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-08/final_model\n",
      "/tmp/ipykernel_27330/2821300086.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "/tmp/ipykernel_27330/2821300086.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
      "/tmp/ipykernel_27330/2821300086.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
      "Preprocessing train data...\n",
      "Checking train dataset...\n",
      "Using Base Trainer\n",
      "\n",
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Created dummy_output_dir/WAE_MMD_training_2023-09-28_07-45-12. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Training params:\n",
      " - max_epochs: 50\n",
      " - per_device_train_batch_size: 6\n",
      " - per_device_eval_batch_size: 1\n",
      " - checkpoint saving every: 300\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.91, 0.995)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ffd54b940>\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/50: 100%|██████████| 1/1 [00:00<00:00,  9.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 72.194\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/50: 100%|██████████| 1/1 [00:00<00:00, 12.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 53.5487\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/50: 100%|██████████| 1/1 [00:00<00:00, 17.81batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 59.0195\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/50: 100%|██████████| 1/1 [00:00<00:00, 17.88batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.5877\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/50: 100%|██████████| 1/1 [00:00<00:00, 11.75batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 91.6884\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/50: 100%|██████████| 1/1 [00:00<00:00, 18.27batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 98.3606\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/50: 100%|██████████| 1/1 [00:00<00:00, 17.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 80.7933\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/50: 100%|██████████| 1/1 [00:00<00:00,  7.37batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 55.568\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/50: 100%|██████████| 1/1 [00:00<00:00, 15.32batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.2469\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/50: 100%|██████████| 1/1 [00:00<00:00, 12.89batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.1943\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/50: 100%|██████████| 1/1 [00:00<00:00, 13.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 102.0953\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/50: 100%|██████████| 1/1 [00:00<00:00, 17.78batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 148.4892\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/50: 100%|██████████| 1/1 [00:00<00:00, 16.80batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.4659\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/50: 100%|██████████| 1/1 [00:00<00:00,  6.51batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 67.2971\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/50: 100%|██████████| 1/1 [00:00<00:00, 16.61batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 65.8675\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/50: 100%|██████████| 1/1 [00:00<00:00, 15.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 79.4155\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/50: 100%|██████████| 1/1 [00:00<00:00, 15.48batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 68.9308\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/50: 100%|██████████| 1/1 [00:00<00:00, 16.92batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 92.4645\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/50: 100%|██████████| 1/1 [00:00<00:00, 17.74batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 104.9272\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/50: 100%|██████████| 1/1 [00:00<00:00,  7.44batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 69.6397\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/50: 100%|██████████| 1/1 [00:00<00:00, 17.41batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 71.3947\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/50: 100%|██████████| 1/1 [00:00<00:00, 15.39batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 76.2049\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/50: 100%|██████████| 1/1 [00:00<00:00, 15.70batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 57.7259\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/50: 100%|██████████| 1/1 [00:00<00:00, 16.73batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 55.0723\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/50: 100%|██████████| 1/1 [00:00<00:00, 14.45batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 89.8745\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/50: 100%|██████████| 1/1 [00:00<00:00, 16.52batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 70.939\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/50: 100%|██████████| 1/1 [00:00<00:00, 18.07batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 108.4296\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 86\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mif test_data.shape[0]!=0:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m    eval_numpy=np.concatenate([generate_data(eval_data,column) for column in train_columns])\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m    eval_numpy=eval_numpy.transpose((1,0,2))\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    #print(train_data.shape,train_numpy.shape,eval_data.shape)\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m pipeline \u001b[39m=\u001b[39m get_model(model_name,dim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_columns),train_batch\u001b[39m=\u001b[39mtrain_numpy\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 86\u001b[0m pipeline(\n\u001b[1;32m     87\u001b[0m train_data\u001b[39m=\u001b[39;49mtrain_numpy\u001b[39m# must be torch.Tensor, np.array or torch datasets\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     89\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel ready!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m my_vae_model\u001b[39m=\u001b[39mpipeline\u001b[39m.\u001b[39mmodel\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pythae/pipelines/training.py:242\u001b[0m, in \u001b[0;36mTrainingPipeline.__call__\u001b[0;34m(self, train_data, eval_data, callbacks)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe provided training config is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m trainer\n\u001b[0;32m--> 242\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pythae/trainers/base_trainer/base_trainer.py:433\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, log_output_dir)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_begin(\n\u001b[1;32m    425\u001b[0m     training_config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_config,\n\u001b[1;32m    426\u001b[0m     epoch\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m    427\u001b[0m     train_loader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader,\n\u001b[1;32m    428\u001b[0m     eval_loader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_loader,\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    431\u001b[0m metrics \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 433\u001b[0m epoch_train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(epoch)\n\u001b[1;32m    434\u001b[0m metrics[\u001b[39m\"\u001b[39m\u001b[39mtrain_epoch_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m epoch_train_loss\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pythae/trainers/base_trainer/base_trainer.py:590\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    588\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 590\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[1;32m    592\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_inputs_to_device(inputs)\n\u001b[1;32m    594\u001b[0m     model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m    595\u001b[0m         inputs,\n\u001b[1;32m    596\u001b[0m         epoch\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m    597\u001b[0m         dataset_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader\u001b[39m.\u001b[39mdataset),\n\u001b[1;32m    598\u001b[0m         uses_ddp\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed,\n\u001b[1;32m    599\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1348\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \n\u001b[1;32m   1353\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1474\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1470\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1471\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1475\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1476\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#files=os.listdir('dataset/train')[0:5]\n",
    "import io\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "captured_output = io.StringIO()\n",
    "files=[\"118.csv\",\"246.csv\",\"1245.csv\",\"1311.csv\",\"1141.csv\"]\n",
    "\"\"\"\n",
    "models=['AE',\n",
    " 'VAE',\n",
    " 'BETA',\n",
    " 'VAE_LinNF',\n",
    " 'VAE_IAF',\n",
    " 'DBVAE',\n",
    " 'IWVAE',\n",
    " 'MIWAE',\n",
    " 'CIWAE']\n",
    "\"\"\"\n",
    "models=['WAE',\n",
    "'INFOVAE',\n",
    " 'VAMP',\n",
    " 'SVAE',\n",
    " 'PVAE',\n",
    " 'VQVAE',\n",
    " 'HVAE',\n",
    " 'RAE_GP',\n",
    " 'RHVAE']\n",
    "pbar=tqdm(total=len(models))\n",
    "try:\n",
    "    for model_name in models:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = captured_output\n",
    "        for file in files:\n",
    "            \n",
    "            dataset=pd.read_csv('dataset/train/'+file)\n",
    "            dataset=prepare_dataset(dataset)\n",
    "            columns_cluster=[\"cycle\",\"trend\"]\n",
    "                        \n",
    "            for column_cluster in columns_cluster:\n",
    "                dataset['kmeans_{}'.format(column_cluster)]=np.nan\n",
    "            for month in dataset[\"months\"].unique():\n",
    "                for day in dataset[\"weekend\"].unique():\n",
    "                        df=dataset[(dataset[\"months\"]==month) & (dataset[\"weekend\"]==day)].copy()\n",
    "                        \n",
    "                        \n",
    "                        for column_cluster in columns_cluster:\n",
    "                                \n",
    "                                df['kmeans_{}'.format(column_cluster)]=get_cluster_labels(df,column_cluster)\n",
    "                                #create a new column with the cluster labels in dataset and initialize it with NAN\n",
    "                                \n",
    "                                \n",
    "                        dataset.update(df)\n",
    "            for column_cluster in columns_cluster:\n",
    "                if not(os.path.exists(\"experiment_2/csv/{}\".format(column_cluster))):\n",
    "                    os.makedirs(\"experiment_2/csv/{}\".format(column_cluster))\n",
    "                train=dataset[dataset['kmeans_{}'.format(column_cluster)]==0.0]\n",
    "                test=dataset[dataset['kmeans_{}'.format(column_cluster)]==1.0]\n",
    "                for month in dataset[\"months\"].unique():\n",
    "                    for day in dataset[\"weekend\"].unique():\n",
    "                        \n",
    "                        train_data=train[(train[\"months\"]==month) & (train[\"weekend\"]==day)]\n",
    "                        test_data=test[(test[\"months\"]==month) & (test[\"weekend\"]==day)]\n",
    "                        train_data.reset_index(inplace=True)\n",
    "                        test_data.reset_index(inplace=True) \n",
    "                        if train_data.shape[0]==0:\n",
    "                            split=0.5\n",
    "                            train_data=test_data[0:int(split*test_data.shape[0])]\n",
    "                            eval_data=test_data[int(split*test_data.shape[0]):]\n",
    "                        else:\n",
    "                            if test_data.shape[0]==0:\n",
    "                                eval_data=None\n",
    "                            else:\n",
    "                                eval_data=test_data\n",
    "                        train_columns=[\"readings\",\"cycle\",\"trend\"] #,\"trend\"\n",
    "                        train_numpy=np.concatenate([generate_data(train_data,column) for column in train_columns])\n",
    "                        train_numpy=train_numpy.transpose((1,0,2))\n",
    "\n",
    "                        \"\"\"\n",
    "                        if test_data.shape[0]!=0:\n",
    "                            eval_numpy=np.concatenate([generate_data(eval_data,column) for column in train_columns])\n",
    "                            eval_numpy=eval_numpy.transpose((1,0,2))\n",
    "                            #print(train_data.shape,train_numpy.shape,eval_data.shape)\"\"\"\n",
    "                        \n",
    "                        pipeline = get_model(model_name,dim=len(train_columns),train_batch=train_numpy.shape[0])\n",
    "                        pipeline(\n",
    "                        train_data=train_numpy# must be torch.Tensor, np.array or torch datasets\n",
    "                        )\n",
    "                        print(\"Model ready!\")\n",
    "                        my_vae_model=pipeline.model\n",
    "                        train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
    "                        train_data.reset_index(inplace=True, drop=True)\n",
    "                        train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
    "\n",
    "                        if test_data.shape[0]!=0:\n",
    "                            eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
    "                            eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
    "                            eval_data.reset_index(inplace=True, drop=True)\n",
    "                        if test_data.shape[0] != 0:\n",
    "                            dataset_final=pd.concat([train_data,eval_data])\n",
    "                        else:\n",
    "                            dataset_final=train_data\n",
    "                        dataset_final[\"reconstruction\"]=dataset_final[\"reconstruction\"].apply((lambda x: x.reshape((1, len(train_columns), 24))))\n",
    "                        dataset_final['difference'] = dataset_final['preprocessed'] - dataset_final['reconstruction']\n",
    "\n",
    "                        for i, column in enumerate(train_columns):\n",
    "                            dataset_final['difference_norm_{}'.format(column)] = dataset_final['difference'].apply(lambda x: np.linalg.norm(x[:,i,:],axis=(0)))\n",
    "                        dataset_final.reset_index(inplace=True,drop=True)\n",
    "                        #dataset_final.to_csv(\"experiment_1/csv/building_{}_month_{}_weekday_{}.csv\".format(file,month,day),index=False)\n",
    "                        for ind in dataset_final.index:\n",
    "                            tmp_df=pd.DataFrame()\n",
    "                            anomalies=dataset_final.loc[ind,\"anomalies\"]\n",
    "                            filled=dataset_final.loc[ind,\"filled\"]\n",
    "                            #tmp_df[\"value\"]=difference_norm\n",
    "                            tmp_df[\"anomalies\"]=anomalies\n",
    "                            tmp_df[\"filled\"]=filled\n",
    "                            tmp_df.reset_index(inplace=True,drop=True)\n",
    "                            date=dataset_final.loc[ind,\"date\"]\n",
    "                            tmp_df['datetime'] = pd.to_datetime(date) + pd.to_timedelta(tmp_df.index, unit='h')\n",
    "\n",
    "\n",
    "                            \n",
    "                            for column in train_columns:\n",
    "                                #plt.figure()\n",
    "                                \n",
    "\n",
    "                                # Plot the first array in blue where the second array has 0\n",
    "                                #plt.plot(x_values[array_of_ones_zeros == 0], array_of_floats[array_of_ones_zeros == 0], c='b', label='0')\n",
    "                                tmp_df[\"difference_norm_{}\".format(column)]=dataset_final.loc[ind,\"difference_norm_{}\".format(column)]\n",
    "                                \n",
    "                                # Plot the first array in red where the second array has 1\n",
    "                                \n",
    "                                #plt.ylim(0, 1)\n",
    "                                #plt.plot(tmp_df[tmp_df[\"anomalies\"]==0][\"value\"], c='b')\n",
    "                                #plt.plot(tmp_df[tmp_df[\"anomalies\"]==1][\"value\"], c='r')\n",
    "                                #plt.plot(tmp_df[tmp_df[\"filled\"]==1][\"value\"], c='y')\n",
    "                                #threshold=0.1*tmp_df[\"value\"].mean()+3*tmp_df[\"value\"].std()\n",
    "                                #threshold_array=np.ones(24)*threshold\n",
    "                                #create a label array that has 1 when the value is above threshold and 0 otherwise\n",
    "                                #label_array = np.where(difference_norm > threshold, 1, 0)\n",
    "                                #tmp_df[\"label\"]=label_array\n",
    "                                \n",
    "                                # create a datetime column that takes the date (yyyy-mm-dd) and adds the corresponding hour (00:00:00 - 23:00:00 according to the index)\n",
    "                                \n",
    "                                \n",
    "                                #plt.plot(threshold_array, c='g')\n",
    "                                #plt.savefig(\"experiment_1/plots/building_{}_month_{}_weekday_{}_{}_column_{}.png\".format(file,month,day,ind,column))\n",
    "                                #plt.close()\n",
    "                            if not(os.path.exists(\"experiment_2/csv/{}/latent_dim_16/{}\".format(column_cluster,model_name))):\n",
    "                                os.makedirs(\"experiment_2/csv/{}/latent_dim_16/{}\".format(column_cluster,model_name))\n",
    "                            tmp_df.to_csv(\"experiment_2/csv/{}/latent_dim_16/{}/building_{}_month_{}_weekend_{}_{}_column_{}.csv\".format(column_cluster,model_name,file,month,day,ind,column),index=False)\n",
    "                clear_output(wait=False)    \n",
    "        sys.stdout = original_stdout\n",
    "        pbar.update(1)\n",
    "except Exception as e:\n",
    "    sys.stdout = original_stdout\n",
    "    print(e)\n",
    "    print(model_name,file,month,day,ind,column)\n",
    "    print(train_data.shape,eval_data.shape)\n",
    "    pbar.close()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE 246.csv 5 0 4 trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_train=train_numpy[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tfsnippet.distributions import Normal\n",
    "from tfsnippet.modules import VAE, Lambda, Module\n",
    "from tfsnippet.stochastic import validate_n_samples\n",
    "from tfsnippet.utils import (VarScopeObject,\n",
    "                             reopen_variable_scope,\n",
    "                             is_integer)\n",
    "from tfsnippet.variational import VariationalInference\n",
    "\n",
    "from .reconstruction import iterative_masked_reconstruct\n",
    "\n",
    "__all__ = ['Donut']\n",
    "\n",
    "\n",
    "def softplus_std(inputs, units, epsilon, name):\n",
    "    return tf.nn.softplus(tf.layers.dense(inputs, units, name=name)) + epsilon\n",
    "\n",
    "\n",
    "def wrap_params_net(inputs, h_for_dist, mean_layer, std_layer):\n",
    "    with tf.variable_scope('hidden'):\n",
    "        h = h_for_dist(inputs)\n",
    "    return {\n",
    "        'mean': mean_layer(h),\n",
    "        'std': std_layer(h),\n",
    "    }\n",
    "\n",
    "\n",
    "class Donut(VarScopeObject):\n",
    "    \"\"\"\n",
    "    Class for constructing Donut model.\n",
    "\n",
    "    This class provides :meth:`get_training_loss` for deriving the\n",
    "    training loss :class:`tf.Tensor`, and :meth:`get_score` for obtaining\n",
    "    the reconstruction probability :class:`tf.Tensor`.\n",
    "\n",
    "    Note:\n",
    "        :class:`Donut` instances will not build the computation graph\n",
    "        until :meth:`get_training_loss` or :meth:`get_score` is\n",
    "        called.  This suggests that a :class:`donut.DonutTrainer` or\n",
    "        a :class:`donut.DonutPredictor` must have been constructed\n",
    "        before saving or restoring the model parameters.\n",
    "\n",
    "    Args:\n",
    "        h_for_p_x (Module or (tf.Tensor) -> tf.Tensor):\n",
    "            The hidden network for :math:`p(x|z)`.\n",
    "        h_for_q_z (Module or (tf.Tensor) -> tf.Tensor):\n",
    "            The hidden network for :math:`q(z|x)`.\n",
    "        x_dims (int): The number of `x` dimensions.\n",
    "        z_dims (int): The number of `z` dimensions.\n",
    "        std_epsilon (float): The minimum value of std for `x` and `z`.\n",
    "        name (str): Optional name of this module\n",
    "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
    "        scope (str): Optional scope of this module\n",
    "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
    "    \"\"\"\n",
    "    def __init__(self, h_for_p_x, h_for_q_z, x_dims, z_dims, std_epsilon=1e-4,\n",
    "                 name=None, scope=None):\n",
    "        if not is_integer(x_dims) or x_dims <= 0:\n",
    "            raise ValueError('`x_dims` must be a positive integer')\n",
    "        if not is_integer(z_dims) or z_dims <= 0:\n",
    "            raise ValueError('`z_dims` must be a positive integer')\n",
    "\n",
    "        super(Donut, self).__init__(name=name, scope=scope)\n",
    "        with reopen_variable_scope(self.variable_scope):\n",
    "            self._vae = VAE(\n",
    "                p_z=Normal(mean=tf.zeros([z_dims]), std=tf.ones([z_dims])),\n",
    "                p_x_given_z=Normal,\n",
    "                q_z_given_x=Normal,\n",
    "                h_for_p_x=Lambda(\n",
    "                    partial(\n",
    "                        wrap_params_net,\n",
    "                        h_for_dist=h_for_p_x,\n",
    "                        mean_layer=partial(\n",
    "                            tf.layers.dense, units=x_dims, name='x_mean'\n",
    "                        ),\n",
    "                        std_layer=partial(\n",
    "                            softplus_std, units=x_dims, epsilon=std_epsilon,\n",
    "                            name='x_std'\n",
    "                        )\n",
    "                    ),\n",
    "                    name='p_x_given_z'\n",
    "                ),\n",
    "                h_for_q_z=Lambda(\n",
    "                    partial(\n",
    "                        wrap_params_net,\n",
    "                        h_for_dist=h_for_q_z,\n",
    "                        mean_layer=partial(\n",
    "                            tf.layers.dense, units=z_dims, name='z_mean'\n",
    "                        ),\n",
    "                        std_layer=partial(\n",
    "                            softplus_std, units=z_dims, epsilon=std_epsilon,\n",
    "                            name='z_std'\n",
    "                        )\n",
    "                    ),\n",
    "                    name='q_z_given_x'\n",
    "                )\n",
    "            )\n",
    "        self._x_dims = x_dims\n",
    "        self._z_dims = z_dims\n",
    "\n",
    "    @property\n",
    "    def x_dims(self):\n",
    "        \"\"\"Get the number of `x` dimensions.\"\"\"\n",
    "        return self._x_dims\n",
    "\n",
    "    @property\n",
    "    def z_dims(self):\n",
    "        \"\"\"Get the number of `z` dimensions.\"\"\"\n",
    "        return self._z_dims\n",
    "\n",
    "    @property\n",
    "    def vae(self):\n",
    "        \"\"\"\n",
    "        Get the VAE object of this :class:`Donut` model.\n",
    "\n",
    "        Returns:\n",
    "            VAE: The VAE object of this model.\n",
    "        \"\"\"\n",
    "        return self._vae\n",
    "\n",
    "    def get_training_loss(self, x, y, n_z=None):\n",
    "        \"\"\"\n",
    "        Get the training loss for `x` and `y`.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
    "                KPI observations in a mini-batch.\n",
    "            y (tf.Tensor): 2-D `int32` :class:`tf.Tensor`, the windows of\n",
    "                ``(label | missing)`` in a mini-batch.\n",
    "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
    "                (default :obj:`None`, one sample without explicit sampling\n",
    "                dimension)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: 0-d tensor, the training loss, which can be optimized\n",
    "                by gradient descent algorithms.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Donut.training_loss'):\n",
    "            chain = self.vae.chain(x, n_z=n_z)\n",
    "            x_log_prob = chain.model['x'].log_prob(group_ndims=0)\n",
    "            alpha = tf.cast(1 - y, dtype=tf.float32)\n",
    "            beta = tf.reduce_mean(alpha, axis=-1)\n",
    "            log_joint = (\n",
    "                tf.reduce_sum(alpha * x_log_prob, axis=-1) +\n",
    "                beta * chain.model['z'].log_prob()\n",
    "            )\n",
    "            vi = VariationalInference(\n",
    "                log_joint=log_joint,\n",
    "                latent_log_probs=chain.vi.latent_log_probs,\n",
    "                axis=chain.vi.axis\n",
    "            )\n",
    "            loss = tf.reduce_mean(vi.training.sgvb())\n",
    "            return loss\n",
    "\n",
    "    def get_training_objective(self, *args, **kwargs):  # pragma: no cover\n",
    "        warnings.warn('`get_training_objective` is deprecated, use '\n",
    "                      '`get_training_loss` instead.', DeprecationWarning)\n",
    "        return self.get_training_loss(*args, **kwargs)\n",
    "\n",
    "    def get_score(self, x, y=None, n_z=None, mcmc_iteration=None,\n",
    "                  last_point_only=True):\n",
    "        \"\"\"\n",
    "        Get the reconstruction probability for `x` and `y`.\n",
    "\n",
    "        The larger `reconstruction probability`, the less likely a point\n",
    "        is anomaly.  You may take the negative of the score, if you want\n",
    "        something to directly indicate the severity of anomaly.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
    "                KPI observations in a mini-batch.\n",
    "            y (tf.Tensor): 2-D `int32` :class:`tf.Tensor`, the windows of\n",
    "                missing point indicators in a mini-batch.\n",
    "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
    "                (default :obj:`None`, one sample without explicit sampling\n",
    "                dimension)\n",
    "            mcmc_iteration (int or tf.Tensor): Iteration count for MCMC\n",
    "                missing data imputation. (default :obj:`None`, no iteration)\n",
    "            last_point_only (bool): Whether to obtain the reconstruction\n",
    "                probability of only the last point in each window?\n",
    "                (default :obj:`True`)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The reconstruction probability, with the shape\n",
    "                ``(len(x) - self.x_dims + 1,)`` if `last_point_only` is\n",
    "                :obj:`True`, or ``(len(x) - self.x_dims + 1, self.x_dims)``\n",
    "                if `last_point_only` is :obj:`False`.  This is because the\n",
    "                first ``self.x_dims - 1`` points are not the last point of\n",
    "                any window.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Donut.get_score'):\n",
    "            # MCMC missing data imputation\n",
    "            if y is not None and mcmc_iteration:\n",
    "                x_r = iterative_masked_reconstruct(\n",
    "                    reconstruct=self.vae.reconstruct,\n",
    "                    x=x,\n",
    "                    mask=y,\n",
    "                    iter_count=mcmc_iteration,\n",
    "                    back_prop=False,\n",
    "                )\n",
    "            else:\n",
    "                x_r = x\n",
    "\n",
    "            # get the reconstruction probability\n",
    "            q_net = self.vae.variational(x=x_r, n_z=n_z)  # notice: x=x_r\n",
    "            p_net = self.vae.model(z=q_net['z'], x=x, n_z=n_z)  # notice: x=x\n",
    "            r_prob = p_net['x'].log_prob(group_ndims=0)\n",
    "            if n_z is not None:\n",
    "                n_z = validate_n_samples(n_z, 'n_z')\n",
    "                assert_shape_op = tf.assert_equal(\n",
    "                    tf.shape(r_prob),\n",
    "                    tf.stack([n_z, tf.shape(x)[0], self.x_dims]),\n",
    "                    message='Unexpected shape of reconstruction prob'\n",
    "                )\n",
    "                with tf.control_dependencies([assert_shape_op]):\n",
    "                    r_prob = tf.reduce_mean(r_prob, axis=0)\n",
    "            if last_point_only:\n",
    "                r_prob = r_prob[:, -1]\n",
    "            return r_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "\n",
    "# We build the entire model within the scope of `model_vs`,\n",
    "# it should hold exactly all the variables of `model`, including\n",
    "# the variables created by Keras layers.\n",
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=120,\n",
    "        z_dims=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import DonutTrainer, DonutPredictor\n",
    "\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs)\n",
    "predictor = DonutPredictor(model)\n",
    "\n",
    "with tf.Session().as_default():\n",
    "    trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "    test_score = predictor.get_score(test_values, test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import iterative_masked_reconstruct\n",
    "\n",
    "# Obtain the reconstructed `x`, with MCMC missing data imputation.\n",
    "# See also:\n",
    "#   :meth:`donut.Donut.get_score`\n",
    "#   :func:`donut.iterative_masked_reconstruct`\n",
    "#   :meth:`tfsnippet.modules.VAE.reconstruct`\n",
    "input_x = ...  # 2-D `float32` :class:`tf.Tensor`, input `x` windows\n",
    "input_y = ...  # 2-D `int32` :class:`tf.Tensor`, missing point indicators\n",
    "               # for the `x` windows\n",
    "x = model.vae.reconstruct(\n",
    "    iterative_masked_reconstruct(\n",
    "        reconstruct=model.vae.reconstruct,\n",
    "        x=input_x,\n",
    "        mask=input_y,\n",
    "        iter_count=mcmc_iteration,\n",
    "        back_prop=False\n",
    "    )\n",
    ")\n",
    "# `x` is a :class:`tfsnippet.stochastic.StochasticTensor`, from which\n",
    "# you may derive many useful outputs, for example:\n",
    "x.tensor  # the `x` samples\n",
    "x.log_prob(group_ndims=0)  # element-wise log p(x|z) of sampled x\n",
    "x.distribution.log_prob(input_x)  # the reconstruction probability\n",
    "x.distribution.mean, x.distribution.std  # mean and std of p(x|z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
