{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "columns=['difference_norm_readings','difference_norm_cycle','difference_norm_trend']\n",
    "\n",
    "models=['AE',\n",
    " 'VAE',\n",
    " 'BETA',\n",
    " 'VAE_LinNF',\n",
    " 'VAE_IAF',\n",
    " 'DBVAE',\n",
    " 'IWVAE',\n",
    " 'MIWAE',\n",
    " 'CIWAE',\n",
    " 'WAE',\n",
    " 'INFOVAE',\n",
    " 'VAMP',\n",
    " 'VQVAE',\n",
    " 'HVAE',\n",
    " 'RAE_GP',\n",
    " 'RHVAE']\n",
    "latent_dim=16\n",
    "experiment=\"experiment_1\"\n",
    "building_df=pd.DataFrame()\n",
    "buildings=[\"118\",\"246\",\"1245\",\"1311\",\"1141\"]\n",
    "pbar=tqdm(total=len(buildings))\n",
    "start=1\n",
    "for building in buildings:\n",
    "    \n",
    "    for model in models:\n",
    "    \n",
    "        #load all .csv files in experiment_1/csv/ into a single dataframe\n",
    "        #make sure that the path is correct, change latent_dim_16 to latent_dim_8 if you want to use the 8 dimensional latent space results\n",
    "        df = pd.concat([pd.read_csv(f) for f in glob.glob('experiment_1/csv/latent_dim_{}/{}/building_{}*.csv'.format(latent_dim,model,building))], ignore_index = True)\n",
    "        df_adjusted = df[df['filled'] == 0]\n",
    "        \n",
    "        if start==1:\n",
    "            \n",
    "            if start==1:\n",
    "                column='difference_norm_readings'\n",
    "                #error=df_adjusted[column]\n",
    "                seuil=df_adjusted[column].mean()+3*df_adjusted[column].std()\n",
    "                df_adjusted['anomaly_{}'.format(column)] = df_adjusted[column] > seuil\n",
    "                df_adjusted['anomaly_{}'.format(column)] = df_adjusted['anomaly_{}'.format(column)].astype(int)\n",
    "                \n",
    "                x=df_adjusted['anomaly_{}'.format(column)]\n",
    "                y=df_adjusted[\"anomalies\"]\n",
    "                precision=precision_score(y, x)\n",
    "                recall=recall_score(y, x)\n",
    "                f1=f1_score(y, x)\n",
    "                accuracy=accuracy_score(y, x)\n",
    "                kappa=cohen_kappa_score(y, x)\n",
    "                #add a new line to the dataframe building_df that contains the metrics\n",
    "                building_df=building_df.append({'building':building,'model':model,'precision':precision,'recall':recall,'f1':f1,'accuracy':accuracy,'kappa':kappa},ignore_index=True)\n",
    "            \n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "if not os.path.exists('experiment_1/result'):\n",
    "    os.makedirs('experiment_1/result')\n",
    "building_df.to_csv('experiment_1/result/result_latent_{}.csv'.format(latent_dim),index=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result visualization example\n",
    "df=pd.read_csv('experiment_1/result/result_latent_{}.csv'.format(latent_dim))\n",
    "df=df[df['building']==118] #building id\n",
    "df=df[['model','precision','recall','f1','kappa']] #metrics\n",
    "df=df[(df['model']=='AE')|(df['model']=='VAE')|(df['model']=='IWVAE')|(df['model']=='WAE')|(df['model']=='VQVAE')|(df['model']=='HVAE')|(df['model']=='RAE_GP')] #models\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "columns=['difference_norm_readings','difference_norm_cycle','difference_norm_trend']\n",
    "models=['AE',\n",
    " 'VAE',\n",
    " 'BETA',\n",
    " 'VAE_LinNF',\n",
    " 'VAE_IAF',\n",
    " 'DBVAE',\n",
    " 'IWVAE',\n",
    " 'MIWAE',\n",
    " 'CIWAE',\n",
    " 'WAE',\n",
    " 'INFOVAE',\n",
    " 'VAMP',\n",
    " 'SVAE',\n",
    " 'PVAE',\n",
    " 'VQVAE',\n",
    " 'HVAE',\n",
    " 'RAE_GP',\n",
    " 'RHVAE']\n",
    "latent_dim=16\n",
    "experiment=\"experiment_2\"\n",
    "\n",
    "buildings=[\"118\",\"246\",\"1245\",\"1311\",\"1141\"]\n",
    "pbar=tqdm(total=len(buildings))\n",
    "start=1\n",
    "columns_cluster=[\"cycle\",\"trend\"]\n",
    "for column_cluster in columns_cluster:\n",
    "    building_df=pd.DataFrame()\n",
    "    for building in buildings:\n",
    "        \n",
    "        for model in models:\n",
    "        \n",
    "            #load all .csv files in experiment_1/csv/ into a single dataframe\n",
    "            df = pd.concat([pd.read_csv(f) for f in glob.glob('{}/csv/{}/latent_dim_{}/{}/building_{}*.csv'.format(experiment,column_cluster,latent_dim,model,building))], ignore_index = True)\n",
    "            df_adjusted = df[df['filled'] == 0]\n",
    "            \n",
    "            if start==1:\n",
    "                \n",
    "                if start==1:\n",
    "                    column='difference_norm_readings'\n",
    "                    seuil=df_adjusted[column].mean()+3*df_adjusted[column].std()\n",
    "                    df_adjusted['anomaly_{}'.format(column)] = df_adjusted[column] > seuil\n",
    "                    df_adjusted['anomaly_{}'.format(column)] = df_adjusted['anomaly_{}'.format(column)].astype(int)\n",
    "                    \n",
    "                    x=df_adjusted['anomaly_{}'.format(column)]\n",
    "                    y=df_adjusted[\"anomalies\"]\n",
    "                    precision=precision_score(y, x)\n",
    "                    recall=recall_score(y, x)\n",
    "                    f1=f1_score(y, x)\n",
    "                    accuracy=accuracy_score(y, x)\n",
    "                    kappa=cohen_kappa_score(y, x)\n",
    "                    #add a new line to the dataframe building_df that contains the metrics\n",
    "                    building_df=building_df.append({'building':building,'model':model,'precision':precision,'recall':recall,'f1':f1,'accuracy':accuracy,'kappa':kappa},ignore_index=True)\n",
    "                \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    if not os.path.exists('experiment_2/result'):\n",
    "        os.makedirs('experiment_2/result')\n",
    "    if not(os.path.exists('{}/result/{}'.format(experiment,column_cluster))):\n",
    "        os.makedirs('{}/result/{}'.format(experiment,column_cluster))\n",
    "    building_df.to_csv('{}/result/{}/result_latent_{}.csv'.format(experiment,column_cluster,latent_dim),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import one class classification metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,cohen_kappa_score\n",
    "#set the threshold for the anomaly detection as an array that has all values between 0.1 and 0.4\n",
    "thresholds = [i/100 for i in range(10,50)]\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "accuracy = []\n",
    "kappa = []\n",
    "for seuil in thresholds:\n",
    "    df_adjusted[\"label\"]=df_adjusted[\"difference_norm_readings\"].apply(lambda x: 1 if x>seuil else 0)\n",
    "    x=df_adjusted[\"label\"]\n",
    "    y=df_adjusted[\"anomalies\"]\n",
    "\n",
    "    precision.append(precision_score(x,y,average=\"macro\"))\n",
    "    recall.append(recall_score(x,y,average=\"macro\"))    \n",
    "    f1.append(f1_score(x,y,average=\"macro\"))\n",
    "    accuracy.append(accuracy_score(x,y))\n",
    "    kappa.append(cohen_kappa_score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil=0.188\n",
    "df_adjusted[\"label\"]=df_adjusted[\"difference_norm_readings\"].apply(lambda x: 1 if x>seuil else 0)\n",
    "x=df_adjusted[\"label\"]\n",
    "y=df_adjusted[\"anomalies\"]\n",
    "\n",
    "print(\"Precision Score: \",precision_score(x,y,average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(x,y,average=\"macro\"))\n",
    "print(\"F1 Score: \",f1_score(x,y,average=\"macro\"))\n",
    "print(\"Accuracy Score: \",accuracy_score(x,y))\n",
    "print(\"Cohen Kappa Score: \",cohen_kappa_score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "metrics=[\"precision\",\"recall\",\"f1\",\"accuracy\",\"kappa\"]\n",
    "metrics_eval={\"precision\":precision,\"recall\":recall,\"f1\":f1,\"accuracy\":accuracy,\"kappa\":kappa}\n",
    "\n",
    "for metric in metrics:\n",
    "    array=np.array(metrics_eval[metric])\n",
    "    plt.plot(thresholds,array,label=metric)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(\"One class classification metrics ({}) for the anomaly detection\".format(metric)  )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalous=df_adjusted[df_adjusted[\"anomalies\"]==1]\n",
    "x=df_anomalous[\"label\"]\n",
    "y=df_anomalous[\"anomalies\"]\n",
    "print(\"Precision Score: \",precision_score(x,y,average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(x,y,average=\"macro\"))\n",
    "print(\"F1 Score: \",f1_score(x,y,average=\"macro\"))\n",
    "print(\"Accuracy Score: \",accuracy_score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
