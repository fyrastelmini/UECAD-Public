{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cynthia/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import minmaxx scaler\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler, StandardScaler,MaxAbsScaler,PowerTransformer,QuantileTransformer\n",
    "import statsmodels.api as sm\n",
    "from get_model import get_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['filled']=df['meter_reading'].apply(lambda x: 0)\n",
    "    existing_hours = df['timestamp'].dt.floor('H').unique()\n",
    "\n",
    "    start_date = df['timestamp'].min().replace(minute=0, second=0)\n",
    "    end_date = df['timestamp'].max().replace(minute=0, second=0)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    all_hours_present = all(hour in existing_hours for hour in date_range)\n",
    "    if not(all_hours_present):\n",
    "        complete_df = pd.DataFrame({'timestamp': date_range})\n",
    "        df = complete_df.merge(df, on='timestamp', how='left')\n",
    "        df['filled']=df['filled'].fillna(1)\n",
    "        df['meter_reading'] = df['meter_reading'].interpolate(method='linear', limit_direction='both')\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    #interpolate the readings\n",
    "    #df['meter_reading'] = df['meter_reading'].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    #apply minmax scaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df['meter_reading'] = scaler.fit_transform(df['meter_reading'].values.reshape(-1,1))\n",
    "    grouped_df = df.groupby(df['timestamp'].dt.date)\n",
    "\n",
    "    # Step 3: Aggregate 'meter_reading' values into a list for each day\n",
    "    aggregated_df = grouped_df.agg({'meter_reading': list, 'anomaly': list, 'filled':list}).reset_index()\n",
    "\n",
    "    # Step 4: Rename columns and sort by date\n",
    "    aggregated_df.columns = ['date', 'readings', 'anomalies','filled']\n",
    "    aggregated_df = aggregated_df.sort_values(by='date')\n",
    "\n",
    "    # Display the aggregated dataframe\n",
    "    aggregated_df[\"length\"] = aggregated_df[\"readings\"].apply(lambda lst: len([x for x in lst if not pd.isna(x)]))\n",
    "    aggregated_df[\"no_anomalies\"] = aggregated_df[\"anomalies\"].apply(lambda x: True if all(val == 0 for val in x) else False)\n",
    "    #aggregated_df.to_csv(\"LEAD/buildings/building_{}.csv\".format(i),index=False)\n",
    "    df=aggregated_df[aggregated_df[\"length\"]==24]\n",
    "    df['cycle'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[0])\n",
    "    df['trend'] = df[\"readings\"].apply(lambda x: sm.tsa.filters.hpfilter(x, 2)[1])\n",
    "    df[\"months\"] = df[\"date\"].apply(lambda x: str(x.month))\n",
    "    df[\"weekday\"] = df[\"date\"].apply(lambda x: str(x.weekday()))\n",
    "    df[\"weekend\"] = df[\"weekday\"].apply(lambda x: 1 if x in [\"5\",\"6\"] else 0)\n",
    "    return df\n",
    "\n",
    "def generate_data(df,column):\n",
    "    data=df[column]\n",
    "    data=np.concatenate(data)\n",
    "    #tmp=data[0:24]\n",
    "    data=data.reshape((1,df.shape[0],24))\n",
    "    #print(data[0,0,:]==tmp)\n",
    "    return data\n",
    "\n",
    "def prepare_line(col1, col2,col3):\n",
    "    columns=[col1, col2,col3]\n",
    "    dataset=[]\n",
    "    for column in columns:\n",
    "        data=np.array(column)\n",
    "        data=data.reshape(1,1,24)\n",
    "        dataset.append(data)\n",
    "    data_array=np.concatenate(dataset)\n",
    "    data_array=np.transpose(data_array, (1,0,2))\n",
    "    #check=np.concatenate([col1,col2,col3])\n",
    "    #check=check.reshape(1,3,24)\n",
    "    #print(check==data_array)\n",
    "    return(data_array)\n",
    "def prepare_line0(col1, col2):\n",
    "    columns=[col1, col2]\n",
    "    dataset=[]\n",
    "    for column in columns:\n",
    "        data=np.array(column)\n",
    "        data=data.reshape(1,1,24)\n",
    "        dataset.append(data)\n",
    "    data_array=np.concatenate(dataset)\n",
    "    data_array=np.transpose(data_array, (1,0,2))\n",
    "    #check=np.concatenate([col1,col2,col3])\n",
    "    #check=check.reshape(1,3,24)\n",
    "    #print(check==data_array)\n",
    "    return(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a StringIO object to capture the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files=os.listdir('dataset/train')[0:5]\n",
    "import io\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "captured_output = io.StringIO()\n",
    "files=[\"118.csv\",\"246.csv\",\"1245.csv\",\"1311.csv\",\"1141.csv\"]\n",
    "\n",
    "models=['AE',\n",
    " 'VAE',\n",
    " 'BETA',\n",
    " 'VAE_LinNF',\n",
    " 'VAE_IAF',\n",
    " 'DBVAE',\n",
    " 'IWVAE',\n",
    " 'MIWAE',\n",
    " 'CIWAE']\n",
    "\"\"\"\n",
    "models=['WAE',\n",
    "'INFOVAE',\n",
    " 'VAMP',\n",
    " 'SVAE']\n",
    "models=[\n",
    " 'VQVAE',\n",
    " 'HVAE',\n",
    " 'RAE_GP',\n",
    " 'RHVAE']\"\"\"\n",
    "models=['AE',\n",
    " 'VAE',\n",
    " 'BETA',\n",
    " 'VAE_LinNF',\n",
    " 'VAE_IAF',\n",
    " 'DBVAE',\n",
    " 'IWVAE',\n",
    " 'MIWAE',\n",
    " 'CIWAE',\n",
    " 'WAE',\n",
    " 'INFOVAE',\n",
    " 'VAMP',\n",
    " 'VQVAE',\n",
    " 'HVAE',\n",
    " 'RAE_GP',\n",
    " 'RHVAE']\n",
    "\n",
    "pbar=tqdm(total=len(models))\n",
    "try:\n",
    "    for model_name in models:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = captured_output\n",
    "        for file in files:\n",
    "            \n",
    "            dataset=pd.read_csv('dataset/train/'+file)\n",
    "            dataset=prepare_dataset(dataset)\n",
    "            train=dataset[dataset[\"no_anomalies\"]==True]\n",
    "            test=dataset[dataset[\"no_anomalies\"]==False]\n",
    "            for month in dataset[\"months\"].unique():\n",
    "                for day in dataset[\"weekend\"].unique():\n",
    "                    \n",
    "                    train_data=train[(train[\"months\"]==month) & (train[\"weekend\"]==day)]\n",
    "                    test_data=test[(test[\"months\"]==month) & (test[\"weekend\"]==day)]\n",
    "                    train_data.reset_index(inplace=True)\n",
    "                    test_data.reset_index(inplace=True) \n",
    "                    if train_data.shape[0]==0:\n",
    "                        split=0.5\n",
    "                        train_data=test_data[0:int(split*test_data.shape[0])]\n",
    "                        eval_data=test_data[int(split*test_data.shape[0]):]\n",
    "                    else:\n",
    "                        if test_data.shape[0]==0:\n",
    "                            eval_data=None\n",
    "                        else:\n",
    "                            eval_data=test_data\n",
    "                    train_columns=[\"readings\",\"cycle\",\"trend\"] #,\"trend\"\n",
    "                    train_numpy=np.concatenate([generate_data(train_data,column) for column in train_columns])\n",
    "                    train_numpy=train_numpy.transpose((1,0,2))\n",
    "\n",
    "                    \"\"\"\n",
    "                    if test_data.shape[0]!=0:\n",
    "                        eval_numpy=np.concatenate([generate_data(eval_data,column) for column in train_columns])\n",
    "                        eval_numpy=eval_numpy.transpose((1,0,2))\n",
    "                        #print(train_data.shape,train_numpy.shape,eval_data.shape)\"\"\"\n",
    "                    \n",
    "                    pipeline = get_model(model_name,dim=len(train_columns),train_batch=train_numpy.shape[0])\n",
    "                    pipeline(\n",
    "                    train_data=train_numpy# must be torch.Tensor, np.array or torch datasets\n",
    "                    )\n",
    "                    print(\"Model ready!\")\n",
    "                    my_vae_model=pipeline.model\n",
    "                    train_data[\"preprocessed\"]=train_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
    "                    train_data.reset_index(inplace=True, drop=True)\n",
    "                    train_data[\"reconstruction\"]=train_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
    "\n",
    "                    if test_data.shape[0]!=0:\n",
    "                        eval_data[\"preprocessed\"]=eval_data[train_columns].apply(lambda row: prepare_line(*row), axis=1)\n",
    "                        eval_data[\"reconstruction\"]=eval_data[\"preprocessed\"].apply((lambda row: my_vae_model.reconstruct(torch.from_numpy(row.astype(np.float32))).detach().numpy()))\n",
    "                        eval_data.reset_index(inplace=True, drop=True)\n",
    "                    if test_data.shape[0] != 0:\n",
    "                        dataset_final=pd.concat([train_data,eval_data])\n",
    "                    else:\n",
    "                        dataset_final=train_data\n",
    "                    dataset_final[\"reconstruction\"]=dataset_final[\"reconstruction\"].apply((lambda x: x.reshape((1, len(train_columns), 24))))\n",
    "                    dataset_final['difference'] = dataset_final['preprocessed'] - dataset_final['reconstruction']\n",
    "\n",
    "                    for i, column in enumerate(train_columns):\n",
    "                        dataset_final['difference_norm_{}'.format(column)] = dataset_final['difference'].apply(lambda x: np.linalg.norm(x[:,i,:],axis=(0)))\n",
    "                    dataset_final.reset_index(inplace=True,drop=True)\n",
    "                    #dataset_final.to_csv(\"experiment_1/csv/building_{}_month_{}_weekday_{}.csv\".format(file,month,day),index=False)\n",
    "                    for ind in dataset_final.index:\n",
    "                        tmp_df=pd.DataFrame()\n",
    "                        anomalies=dataset_final.loc[ind,\"anomalies\"]\n",
    "                        filled=dataset_final.loc[ind,\"filled\"]\n",
    "                        #tmp_df[\"value\"]=difference_norm\n",
    "                        tmp_df[\"anomalies\"]=anomalies\n",
    "                        tmp_df[\"filled\"]=filled\n",
    "                        tmp_df.reset_index(inplace=True,drop=True)\n",
    "                        date=dataset_final.loc[ind,\"date\"]\n",
    "                        tmp_df['datetime'] = pd.to_datetime(date) + pd.to_timedelta(tmp_df.index, unit='h')\n",
    "\n",
    "\n",
    "                        \n",
    "                        for column in train_columns:\n",
    "                            #plt.figure()\n",
    "                            \n",
    "\n",
    "                            # Plot the first array in blue where the second array has 0\n",
    "                            #plt.plot(x_values[array_of_ones_zeros == 0], array_of_floats[array_of_ones_zeros == 0], c='b', label='0')\n",
    "                            tmp_df[\"difference_norm_{}\".format(column)]=dataset_final.loc[ind,\"difference_norm_{}\".format(column)]\n",
    "                            \n",
    "                            # Plot the first array in red where the second array has 1\n",
    "                            \n",
    "                            #plt.ylim(0, 1)\n",
    "                            #plt.plot(tmp_df[tmp_df[\"anomalies\"]==0][\"value\"], c='b')\n",
    "                            #plt.plot(tmp_df[tmp_df[\"anomalies\"]==1][\"value\"], c='r')\n",
    "                            #plt.plot(tmp_df[tmp_df[\"filled\"]==1][\"value\"], c='y')\n",
    "                            #threshold=0.1*tmp_df[\"value\"].mean()+3*tmp_df[\"value\"].std()\n",
    "                            #threshold_array=np.ones(24)*threshold\n",
    "                            #create a label array that has 1 when the value is above threshold and 0 otherwise\n",
    "                            #label_array = np.where(difference_norm > threshold, 1, 0)\n",
    "                            #tmp_df[\"label\"]=label_array\n",
    "                            \n",
    "                            # create a datetime column that takes the date (yyyy-mm-dd) and adds the corresponding hour (00:00:00 - 23:00:00 according to the index)\n",
    "                            \n",
    "                            \n",
    "                            #plt.plot(threshold_array, c='g')\n",
    "                            #plt.savefig(\"experiment_1/plots/building_{}_month_{}_weekday_{}_{}_column_{}.png\".format(file,month,day,ind,column))\n",
    "                            #plt.close()\n",
    "                        if not(os.path.exists(\"experiment_1/csv/latent_dim_16/{}\".format(model_name))):\n",
    "                            os.makedirs(\"experiment_1/csv/latent_dim_16/{}\".format(model_name))\n",
    "                        tmp_df.to_csv(\"experiment_1/csv/latent_dim_16/{}/building_{}_month_{}_weekend_{}_{}_column_{}.csv\".format(model_name,file,month,day,ind,column),index=False)\n",
    "                clear_output(wait=False)    \n",
    "        sys.stdout = original_stdout\n",
    "        pbar.update(1)\n",
    "except Exception as e:\n",
    "    sys.stdout = original_stdout\n",
    "    print(e)\n",
    "    print(model_name,file,month,day,ind,column)\n",
    "    print(train_data.shape,eval_data.shape)\n",
    "    pbar.close()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE 246.csv 5 0 4 trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_train=train_numpy[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tfsnippet.distributions import Normal\n",
    "from tfsnippet.modules import VAE, Lambda, Module\n",
    "from tfsnippet.stochastic import validate_n_samples\n",
    "from tfsnippet.utils import (VarScopeObject,\n",
    "                             reopen_variable_scope,\n",
    "                             is_integer)\n",
    "from tfsnippet.variational import VariationalInference\n",
    "\n",
    "from .reconstruction import iterative_masked_reconstruct\n",
    "\n",
    "__all__ = ['Donut']\n",
    "\n",
    "\n",
    "def softplus_std(inputs, units, epsilon, name):\n",
    "    return tf.nn.softplus(tf.layers.dense(inputs, units, name=name)) + epsilon\n",
    "\n",
    "\n",
    "def wrap_params_net(inputs, h_for_dist, mean_layer, std_layer):\n",
    "    with tf.variable_scope('hidden'):\n",
    "        h = h_for_dist(inputs)\n",
    "    return {\n",
    "        'mean': mean_layer(h),\n",
    "        'std': std_layer(h),\n",
    "    }\n",
    "\n",
    "\n",
    "class Donut(VarScopeObject):\n",
    "    \"\"\"\n",
    "    Class for constructing Donut model.\n",
    "\n",
    "    This class provides :meth:`get_training_loss` for deriving the\n",
    "    training loss :class:`tf.Tensor`, and :meth:`get_score` for obtaining\n",
    "    the reconstruction probability :class:`tf.Tensor`.\n",
    "\n",
    "    Note:\n",
    "        :class:`Donut` instances will not build the computation graph\n",
    "        until :meth:`get_training_loss` or :meth:`get_score` is\n",
    "        called.  This suggests that a :class:`donut.DonutTrainer` or\n",
    "        a :class:`donut.DonutPredictor` must have been constructed\n",
    "        before saving or restoring the model parameters.\n",
    "\n",
    "    Args:\n",
    "        h_for_p_x (Module or (tf.Tensor) -> tf.Tensor):\n",
    "            The hidden network for :math:`p(x|z)`.\n",
    "        h_for_q_z (Module or (tf.Tensor) -> tf.Tensor):\n",
    "            The hidden network for :math:`q(z|x)`.\n",
    "        x_dims (int): The number of `x` dimensions.\n",
    "        z_dims (int): The number of `z` dimensions.\n",
    "        std_epsilon (float): The minimum value of std for `x` and `z`.\n",
    "        name (str): Optional name of this module\n",
    "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
    "        scope (str): Optional scope of this module\n",
    "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
    "    \"\"\"\n",
    "    def __init__(self, h_for_p_x, h_for_q_z, x_dims, z_dims, std_epsilon=1e-4,\n",
    "                 name=None, scope=None):\n",
    "        if not is_integer(x_dims) or x_dims <= 0:\n",
    "            raise ValueError('`x_dims` must be a positive integer')\n",
    "        if not is_integer(z_dims) or z_dims <= 0:\n",
    "            raise ValueError('`z_dims` must be a positive integer')\n",
    "\n",
    "        super(Donut, self).__init__(name=name, scope=scope)\n",
    "        with reopen_variable_scope(self.variable_scope):\n",
    "            self._vae = VAE(\n",
    "                p_z=Normal(mean=tf.zeros([z_dims]), std=tf.ones([z_dims])),\n",
    "                p_x_given_z=Normal,\n",
    "                q_z_given_x=Normal,\n",
    "                h_for_p_x=Lambda(\n",
    "                    partial(\n",
    "                        wrap_params_net,\n",
    "                        h_for_dist=h_for_p_x,\n",
    "                        mean_layer=partial(\n",
    "                            tf.layers.dense, units=x_dims, name='x_mean'\n",
    "                        ),\n",
    "                        std_layer=partial(\n",
    "                            softplus_std, units=x_dims, epsilon=std_epsilon,\n",
    "                            name='x_std'\n",
    "                        )\n",
    "                    ),\n",
    "                    name='p_x_given_z'\n",
    "                ),\n",
    "                h_for_q_z=Lambda(\n",
    "                    partial(\n",
    "                        wrap_params_net,\n",
    "                        h_for_dist=h_for_q_z,\n",
    "                        mean_layer=partial(\n",
    "                            tf.layers.dense, units=z_dims, name='z_mean'\n",
    "                        ),\n",
    "                        std_layer=partial(\n",
    "                            softplus_std, units=z_dims, epsilon=std_epsilon,\n",
    "                            name='z_std'\n",
    "                        )\n",
    "                    ),\n",
    "                    name='q_z_given_x'\n",
    "                )\n",
    "            )\n",
    "        self._x_dims = x_dims\n",
    "        self._z_dims = z_dims\n",
    "\n",
    "    @property\n",
    "    def x_dims(self):\n",
    "        \"\"\"Get the number of `x` dimensions.\"\"\"\n",
    "        return self._x_dims\n",
    "\n",
    "    @property\n",
    "    def z_dims(self):\n",
    "        \"\"\"Get the number of `z` dimensions.\"\"\"\n",
    "        return self._z_dims\n",
    "\n",
    "    @property\n",
    "    def vae(self):\n",
    "        \"\"\"\n",
    "        Get the VAE object of this :class:`Donut` model.\n",
    "\n",
    "        Returns:\n",
    "            VAE: The VAE object of this model.\n",
    "        \"\"\"\n",
    "        return self._vae\n",
    "\n",
    "    def get_training_loss(self, x, y, n_z=None):\n",
    "        \"\"\"\n",
    "        Get the training loss for `x` and `y`.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
    "                KPI observations in a mini-batch.\n",
    "            y (tf.Tensor): 2-D `int32` :class:`tf.Tensor`, the windows of\n",
    "                ``(label | missing)`` in a mini-batch.\n",
    "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
    "                (default :obj:`None`, one sample without explicit sampling\n",
    "                dimension)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: 0-d tensor, the training loss, which can be optimized\n",
    "                by gradient descent algorithms.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Donut.training_loss'):\n",
    "            chain = self.vae.chain(x, n_z=n_z)\n",
    "            x_log_prob = chain.model['x'].log_prob(group_ndims=0)\n",
    "            alpha = tf.cast(1 - y, dtype=tf.float32)\n",
    "            beta = tf.reduce_mean(alpha, axis=-1)\n",
    "            log_joint = (\n",
    "                tf.reduce_sum(alpha * x_log_prob, axis=-1) +\n",
    "                beta * chain.model['z'].log_prob()\n",
    "            )\n",
    "            vi = VariationalInference(\n",
    "                log_joint=log_joint,\n",
    "                latent_log_probs=chain.vi.latent_log_probs,\n",
    "                axis=chain.vi.axis\n",
    "            )\n",
    "            loss = tf.reduce_mean(vi.training.sgvb())\n",
    "            return loss\n",
    "\n",
    "    def get_training_objective(self, *args, **kwargs):  # pragma: no cover\n",
    "        warnings.warn('`get_training_objective` is deprecated, use '\n",
    "                      '`get_training_loss` instead.', DeprecationWarning)\n",
    "        return self.get_training_loss(*args, **kwargs)\n",
    "\n",
    "    def get_score(self, x, y=None, n_z=None, mcmc_iteration=None,\n",
    "                  last_point_only=True):\n",
    "        \"\"\"\n",
    "        Get the reconstruction probability for `x` and `y`.\n",
    "\n",
    "        The larger `reconstruction probability`, the less likely a point\n",
    "        is anomaly.  You may take the negative of the score, if you want\n",
    "        something to directly indicate the severity of anomaly.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
    "                KPI observations in a mini-batch.\n",
    "            y (tf.Tensor): 2-D `int32` :class:`tf.Tensor`, the windows of\n",
    "                missing point indicators in a mini-batch.\n",
    "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
    "                (default :obj:`None`, one sample without explicit sampling\n",
    "                dimension)\n",
    "            mcmc_iteration (int or tf.Tensor): Iteration count for MCMC\n",
    "                missing data imputation. (default :obj:`None`, no iteration)\n",
    "            last_point_only (bool): Whether to obtain the reconstruction\n",
    "                probability of only the last point in each window?\n",
    "                (default :obj:`True`)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The reconstruction probability, with the shape\n",
    "                ``(len(x) - self.x_dims + 1,)`` if `last_point_only` is\n",
    "                :obj:`True`, or ``(len(x) - self.x_dims + 1, self.x_dims)``\n",
    "                if `last_point_only` is :obj:`False`.  This is because the\n",
    "                first ``self.x_dims - 1`` points are not the last point of\n",
    "                any window.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Donut.get_score'):\n",
    "            # MCMC missing data imputation\n",
    "            if y is not None and mcmc_iteration:\n",
    "                x_r = iterative_masked_reconstruct(\n",
    "                    reconstruct=self.vae.reconstruct,\n",
    "                    x=x,\n",
    "                    mask=y,\n",
    "                    iter_count=mcmc_iteration,\n",
    "                    back_prop=False,\n",
    "                )\n",
    "            else:\n",
    "                x_r = x\n",
    "\n",
    "            # get the reconstruction probability\n",
    "            q_net = self.vae.variational(x=x_r, n_z=n_z)  # notice: x=x_r\n",
    "            p_net = self.vae.model(z=q_net['z'], x=x, n_z=n_z)  # notice: x=x\n",
    "            r_prob = p_net['x'].log_prob(group_ndims=0)\n",
    "            if n_z is not None:\n",
    "                n_z = validate_n_samples(n_z, 'n_z')\n",
    "                assert_shape_op = tf.assert_equal(\n",
    "                    tf.shape(r_prob),\n",
    "                    tf.stack([n_z, tf.shape(x)[0], self.x_dims]),\n",
    "                    message='Unexpected shape of reconstruction prob'\n",
    "                )\n",
    "                with tf.control_dependencies([assert_shape_op]):\n",
    "                    r_prob = tf.reduce_mean(r_prob, axis=0)\n",
    "            if last_point_only:\n",
    "                r_prob = r_prob[:, -1]\n",
    "            return r_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "\n",
    "# We build the entire model within the scope of `model_vs`,\n",
    "# it should hold exactly all the variables of `model`, including\n",
    "# the variables created by Keras layers.\n",
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=120,\n",
    "        z_dims=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import DonutTrainer, DonutPredictor\n",
    "\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs)\n",
    "predictor = DonutPredictor(model)\n",
    "\n",
    "with tf.Session().as_default():\n",
    "    trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "    test_score = predictor.get_score(test_values, test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import iterative_masked_reconstruct\n",
    "\n",
    "# Obtain the reconstructed `x`, with MCMC missing data imputation.\n",
    "# See also:\n",
    "#   :meth:`donut.Donut.get_score`\n",
    "#   :func:`donut.iterative_masked_reconstruct`\n",
    "#   :meth:`tfsnippet.modules.VAE.reconstruct`\n",
    "input_x = ...  # 2-D `float32` :class:`tf.Tensor`, input `x` windows\n",
    "input_y = ...  # 2-D `int32` :class:`tf.Tensor`, missing point indicators\n",
    "               # for the `x` windows\n",
    "x = model.vae.reconstruct(\n",
    "    iterative_masked_reconstruct(\n",
    "        reconstruct=model.vae.reconstruct,\n",
    "        x=input_x,\n",
    "        mask=input_y,\n",
    "        iter_count=mcmc_iteration,\n",
    "        back_prop=False\n",
    "    )\n",
    ")\n",
    "# `x` is a :class:`tfsnippet.stochastic.StochasticTensor`, from which\n",
    "# you may derive many useful outputs, for example:\n",
    "x.tensor  # the `x` samples\n",
    "x.log_prob(group_ndims=0)  # element-wise log p(x|z) of sampled x\n",
    "x.distribution.log_prob(input_x)  # the reconstruction probability\n",
    "x.distribution.mean, x.distribution.std  # mean and std of p(x|z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
